{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1223e065-428f-4535-8bc3-85dc1affdd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.4.1\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is enabled\n",
    "print(torch.__version__)  # Check the installed PyTorch version\n",
    "print(torch.version.cuda)  # Check CUDA version used by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d11f6a0a-2839-46a1-8b97-0472d1099fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ====================================\n",
    "# Fourier Time Embeddings (NCSN++-Style)\n",
    "# ====================================\n",
    "class FourierTimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=64, scale=1.0):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(embedding_size // 2) * scale, requires_grad=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedding_size, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        t_proj = t[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        emb = torch.cat([torch.sin(t_proj), torch.cos(t_proj)], dim=-1)\n",
    "        return self.mlp(emb)\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        device = timesteps.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0, device=device)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb\n",
    "\n",
    "class SelfAttention2D(nn.Module):\n",
    "    def __init__(self, channels, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(8, channels)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=channels, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_norm = self.norm(x)\n",
    "        x_reshaped = x_norm.view(B, C, -1).permute(0, 2, 1)\n",
    "        attn_out, _ = self.attn(x_reshaped, x_reshaped, x_reshaped)\n",
    "        attn_out = attn_out.permute(0, 2, 1).view(B, C, H, W)\n",
    "        return x + attn_out\n",
    "\n",
    "class ResidualBlock2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim, use_attention=False):\n",
    "        super().__init__()\n",
    "        num_groups = min(32, in_channels // 4)\n",
    "        self.norm1 = nn.GroupNorm(num_groups, in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        num_groups = min(32, out_channels // 4)\n",
    "        self.norm2 = nn.GroupNorm(num_groups, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            self.attention = SelfAttention2D(out_channels)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        t_emb = self.time_mlp(F.silu(temb)).view(temb.shape[0], -1, 1, 1)\n",
    "        h = h + t_emb\n",
    "        \n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            h = self.attention(h)\n",
    "        \n",
    "        return h + self.residual(x)\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, out_channels=1, time_emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_emb = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim),\n",
    "        )\n",
    "        self.time_embedding = FourierTimeEmbedding(time_emb_dim)\n",
    "        self.init_conv = nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.down1 = ResidualBlock2D(base_channels, base_channels * 2, time_emb_dim, use_attention=False)\n",
    "        self.down2 = ResidualBlock2D(base_channels * 2, base_channels * 4, time_emb_dim, use_attention=True)\n",
    "        \n",
    "        self.bottleneck = ResidualBlock2D(base_channels * 4, base_channels * 4, time_emb_dim, use_attention=True)\n",
    "        \n",
    "        self.up1 = ResidualBlock2D(base_channels * 4, base_channels * 2, time_emb_dim, use_attention=True)\n",
    "        self.up2 = ResidualBlock2D(base_channels * 2, base_channels, time_emb_dim, use_attention=False)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(base_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        temb = self.time_embedding(t)\n",
    "        x = self.init_conv(x)\n",
    "        x1 = self.down1(x, temb)\n",
    "        x2 = self.down2(x1, temb)\n",
    "\n",
    "        x = self.bottleneck(x2, temb)\n",
    "        x = self.up1(x + x2, temb)\n",
    "        x = self.up2(x + x1, temb)\n",
    "\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b369dfbd-8a19-42a0-a30a-c31b8dc6eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(model_pas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12253c2-83f4-4493-a833-fffd0b97b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 2. Function to Sample Noisy Data at Time t\n",
    "# ====================================\n",
    "def sample_data_at_time_t(x0, T, t):\n",
    "    t = t.view(-1, 1, 1)  # Reshape for broadcasting\n",
    "    mean_t = x0 * torch.exp(-t)  # Mean at time t\n",
    "    var_t = T * (1 - torch.exp(-2 * t)) + 1e-8  # ðŸ”¥ Prevents division by zero\n",
    "    # Sample from Normal distribution\n",
    "    noise = torch.randn_like(x0) * torch.sqrt(var_t)\n",
    "    x_t = mean_t + noise\n",
    "    return x_t\n",
    "\n",
    "# 3. Def Loss Function\n",
    "def diffusion_loss(model, x0, T, t):\n",
    "    device = \"cuda\"  # Ensure everything is on the same device\n",
    "    x0 = x0.to(device)\n",
    "    # Reshape t to broadcast correctly (B, 1, 1, 1) for MNIST images (B, 1, 28, 28)\n",
    "    t = t.view(-1, 1, 1, 1).to(device)\n",
    "\n",
    "    # Compute mean and variance at time t\n",
    "    mean_t = x0 * torch.exp(-t)  # Mean at time t\n",
    "    var_t = T * (1 - torch.exp(-2 * t)) + 1e-8  # Avoid division by zero\n",
    "    var_t = var_t.view(-1, 1, 1, 1)  # Ensure proper shape for broadcasting\n",
    "\n",
    "    # Sample noise from a standard normal distribution\n",
    "    batch_randn = torch.randn_like(x0).to(device)\n",
    "    noise = batch_randn * torch.sqrt(var_t)\n",
    "\n",
    "    # Sample noisy image at time t\n",
    "    xt = mean_t + noise\n",
    "\n",
    "    # Compute correction term\n",
    "    l = batch_randn  # Since noise is added, the true noise should match this\n",
    "\n",
    "    # Compute loss\n",
    "    with torch.amp.autocast(\"cuda\"):  # Use mixed precision for efficiency\n",
    "        pred_noise = model(xt, t)\n",
    "        loss = (-torch.sqrt(var_t) * pred_noise - l) ** 2\n",
    "        loss = torch.mean(torch.sum(loss.reshape(loss.shape[0], -1), dim=-1))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825f32f8-a5e2-49a9-910e-259c6603ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 1. Generate Correlated Gaussian Noise\n",
    "# ====================================\n",
    "def generate_correlated_noise(covariance_matrix, image_shape):\n",
    "    \"\"\"\n",
    "    Generate correlated Gaussian noise for MNIST-shaped data.\n",
    "    Args:\n",
    "        covariance_matrix (torch.Tensor): A [batch_size, 2, 2] covariance matrix.\n",
    "        image_shape (tuple): Target image shape, e.g., (1, 28, 28).\n",
    "    Returns:\n",
    "        torch.Tensor: Correlated noise of shape [batch_size, 2, 28, 28].\n",
    "    \"\"\"\n",
    "    batch_size = covariance_matrix.shape[0]\n",
    "    height, width = image_shape[-2], image_shape[-1]  # Extract MNIST dimensions\n",
    "    covariance_matrix = covariance_matrix.view(batch_size, 2, 2)  # Explicitly reshape it\n",
    "    # Perform Cholesky decomposition\n",
    "    L = torch.linalg.cholesky(covariance_matrix)  # Shape: [batch_size, 2, 2]\n",
    "\n",
    "    # Generate uncorrelated standard normal noise with MNIST shape\n",
    "    uncorrelated_noise = torch.randn(batch_size, 2, height, width).to(covariance_matrix.device)  # [B, 2, 28, 28]\n",
    "\n",
    "    # Reshape for batch matrix multiplication (bmm requires 3D tensors)\n",
    "    uncorrelated_noise = uncorrelated_noise.view(batch_size, 2, -1)  # Reshape to [B, 2, 784]\n",
    "\n",
    "    # Apply the Cholesky factor to obtain correlated noise\n",
    "    correlated_noise = torch.bmm(L, uncorrelated_noise)  # Shape: [B, 2, 784]\n",
    "\n",
    "    # Reshape back to image format\n",
    "    correlated_noise = correlated_noise.view(batch_size, 2, height, width)  # [B, 2, 28, 28]\n",
    "\n",
    "    return correlated_noise\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# 2. Compute Covariance Matrix Components\n",
    "# ====================================\n",
    "def compute_covariance(Tp, Ta, tau, k, t):\n",
    "    a = torch.exp(-k * t)\n",
    "    b = torch.exp(-t / tau)\n",
    "    Tx = Tp\n",
    "    Ty = Ta / (tau * tau)\n",
    "    w = 1 / tau\n",
    "    M11 = (1/k)*Tx*(1-a*a) + (1/k)*Ty*( 1/(w*(k+w)) + 4*a*b*k/((k+w)*(k-w)**2) - (k*b*b + w*a*a)/(w*(k-w)**2) )\n",
    "    M12 = (Ty/(w*(k*k - w*w))) * ( k*(1-b*b) - w*(1 + b*b - 2*a*b) )\n",
    "    M22 = (Ty/w)*(1-b*b)\n",
    "    # Stack elements correctly to match batch dimension\n",
    "    cov_matrix = torch.stack([\n",
    "        torch.cat([M11, M12], dim=1),  # First row: [M11, M12]\n",
    "        torch.cat([M12, M22], dim=1)   # Second row: [M12, M22]\n",
    "    ], dim=1)  # Stack along second dimension to form [batch, 2, 2]\n",
    "    return cov_matrix  # Shape: (batch_size, 2, 2)\n",
    "\n",
    "# ====================================\n",
    "# 3. Sample Noisy Data at Time t\n",
    "# ====================================\n",
    "def sample_data_active(x0, eta0, t, Tp, Ta, tau, k):\n",
    "    device = x0.device  # Ensure everything stays on the correct device\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    cov_mat = compute_covariance(Tp, Ta, tau, k, t)  # Shape: [batch_size, 2, 2]\n",
    "\n",
    "    # Compute mean values at time t\n",
    "    a = torch.exp(-k * t)\n",
    "    b = (torch.exp(-t / tau) - torch.exp(-k * t)) / (k - (1 / tau))\n",
    "    c = torch.exp(-t / tau)\n",
    "\n",
    "    mean_x = a * x0 + b * eta0  # Shape: [batch_size, 1, 28, 28]\n",
    "    mean_eta = c * eta0  # Shape: [batch_size, 1, 28, 28]\n",
    "\n",
    "    # Generate correlated Gaussian noise\n",
    "    noise = generate_correlated_noise(cov_mat, x0.shape[-3:])  # Shape: [batch_size, 2, 28, 28]\n",
    "\n",
    "    # Extract noise for x and eta\n",
    "    noise_x = noise[:, 0, :, :].unsqueeze(1)  # Shape: [batch_size, 1, 28, 28]\n",
    "    noise_eta = noise[:, 1, :, :].unsqueeze(1)  # Shape: [batch_size, 1, 28, 28]\n",
    "\n",
    "    return mean_x + noise_x, mean_eta + noise_eta, noise\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# 4. Define Loss Function\n",
    "# ====================================\n",
    "\n",
    "def diffusion_loss_active(model_eta, x0, eta0, Tp, Ta, tau, k, t):\n",
    "    device = \"cuda\"  # Ensure everything is on the same device\n",
    "    x0, eta0 = x0.to(device), eta0.to(device) \n",
    "    t = t.view(-1, 1, 1, 1).to(device)  # Reshape t properly\n",
    "    Tp, Ta, tau, k = Tp.to(device), Ta.to(device), tau.to(device), k.to(device)\n",
    "    \n",
    "    # Sample noisy data\n",
    "    xt, etat, batch_randn = sample_data_active(x0, eta0, t, Tp, Ta, tau, k)\n",
    "    \n",
    "    # Concatenate along the channel dimension\n",
    "    xin = torch.cat((xt, etat), dim=1).float()  # Assuming input shape is [B, C, H, W]\n",
    "    \n",
    "    # Compute mean adjustments\n",
    "    a = torch.exp(-k * t)\n",
    "    b = (torch.exp(-t / tau) - torch.exp(-k * t)) / (k - (1 / tau))\n",
    "    c = torch.exp(-t / tau)\n",
    "    \n",
    "    # Compute covariance matrix components\n",
    "    M = compute_covariance(Tp, Ta, tau, k, t)\n",
    "    M11 = M[:, 0, 0].view(-1, 1, 1, 1)\n",
    "    M12 = M[:, 0, 1].view(-1, 1, 1, 1)\n",
    "    M22 = M[:, 1, 1].view(-1, 1, 1, 1)\n",
    "    det = M11 * M22 - M12 * M12\n",
    "    \n",
    "    a, b, c = a.view(-1, 1, 1, 1), b.view(-1, 1, 1, 1), c.view(-1, 1, 1, 1)\n",
    "    \n",
    "    # Compute loss terms\n",
    "    Feta = torch.sqrt(1 / det) * (-M11 * (etat - c * eta0) + M12 * (xt - a * x0 - b * eta0))\n",
    "    scr_eta = torch.sqrt(det) * model_eta(xin, t)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_eta = torch.mean((scr_eta - Feta) ** 2)\n",
    "    \n",
    "    return loss_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "487b3ca8-7d70-4f45-8829-c00314140130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfname_pas = \"Passive_models_T_{}\".format(T_var)\\nfname_act = \"Active_models_Ta_{}_tau_{}\".format(Ta_var, tau_var)\\nif not os.path.exists(fname_pas):\\n    os.mkdir(fname_pas)\\nelse:\\n    if os.path.exists(\\'{}/model_at_epoch_{}.pth\\'.format(fname_pas, restart_epoch)):\\n        checkpoint_pas = torch.load(\\'{}/model_at_epoch_{}.pth\\'.format(fname_pas, restart_epoch), weights_only=True)\\n        model_pas.load_state_dict(checkpoint_pas[\\'model_state_dict\\'])\\n        print(\"Passive model loaded\")\\n        \\nif not os.path.exists(fname_act):\\n    os.mkdir(fname_act)\\nelse:\\n    if os.path.exists(\\'{}/model_at_epoch_{}.pth\\'.format(fname_act, restart_epoch)):\\n        checkpoint_act = torch.load(\\'{}/model_at_epoch_{}.pth\\'.format(fname_act, restart_epoch), weights_only=True)\\n        model_act.load_state_dict(checkpoint_act[\\'model_state_dict\\'])\\n        print(\"Active model loaded\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restart_epoch = 0\n",
    "T_var = 1.0\n",
    "Tp_var = 1e-3\n",
    "Ta_var = 1.0\n",
    "tau_var = 0.5\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_pas = UNet2D(in_channels=1, base_channels=16, out_channels=1)\n",
    "model_pas = model_pas.to(device)\n",
    "model_act = UNet2D(in_channels=2, base_channels=16, out_channels=1)\n",
    "model_act = model_act.to(device)\n",
    "\n",
    "\n",
    "fname_pas = \"Passive_models_T_{}\".format(T_var)\n",
    "fname_act = \"Active_models_Ta_{}_tau_{}\".format(Ta_var, tau_var)\n",
    "if not os.path.exists(fname_pas):\n",
    "    os.mkdir(fname_pas)\n",
    "else:\n",
    "    if os.path.exists('{}/model_at_epoch_{}.pth'.format(fname_pas, restart_epoch)):\n",
    "        checkpoint_pas = torch.load('{}/model_at_epoch_{}.pth'.format(fname_pas, restart_epoch), weights_only=True)\n",
    "        model_pas.load_state_dict(checkpoint_pas['model_state_dict'])\n",
    "        print(\"Passive model loaded\")\n",
    "        \n",
    "if not os.path.exists(fname_act):\n",
    "    os.mkdir(fname_act)\n",
    "else:\n",
    "    if os.path.exists('{}/model_at_epoch_{}.pth'.format(fname_act, restart_epoch)):\n",
    "        checkpoint_act = torch.load('{}/model_at_epoch_{}.pth'.format(fname_act, restart_epoch), weights_only=True)\n",
    "        model_act.load_state_dict(checkpoint_act['model_state_dict'])\n",
    "        print(\"Active model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1dbd32-36cd-447d-a7d3-91e99c2e78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "restart_epoch = 0\n",
    "T_var = 1.0\n",
    "Tp_var = 1e-3\n",
    "Ta_var = 1.0\n",
    "tau_var = 0.5\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model_pas = UNet2D(in_channels=1, base_channels=16, out_channels=1)\n",
    "model_pas = model_pas.to(device)\n",
    "model_act = UNet2D(in_channels=2, base_channels=16, out_channels=1)\n",
    "model_act = model_act.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f223f9-6410-457c-a55e-a802b6f96b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define transforms (convert images to tensor and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to [0,1] tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1,1]\n",
    "])\n",
    "\n",
    "# Download and load the dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d385a340-1b62-4679-9db9-6d1adc220db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a69cbf3c-1d89-4a03-948f-e397370bcfcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10, 0/938], Intermidiate Loss, Passive: 862.3075561523438, Active: 0.19194954633712769\n",
      "Epoch [0/10, 1/938], Intermidiate Loss, Passive: 866.6494750976562, Active: 0.3731131851673126\n",
      "Epoch [0/10, 2/938], Intermidiate Loss, Passive: 320.67596435546875, Active: 0.12253031134605408\n",
      "Epoch [0/10, 3/938], Intermidiate Loss, Passive: 352.5382385253906, Active: 0.11905665695667267\n",
      "Epoch [0/10, 4/938], Intermidiate Loss, Passive: 293.0059814453125, Active: 0.12519028782844543\n",
      "Epoch [0/10, 5/938], Intermidiate Loss, Passive: 258.12744140625, Active: 0.08196710795164108\n",
      "Epoch [0/10, 6/938], Intermidiate Loss, Passive: 253.77542114257812, Active: 0.08357680588960648\n",
      "Epoch [0/10, 7/938], Intermidiate Loss, Passive: 228.73153686523438, Active: 0.10557541996240616\n",
      "Epoch [0/10, 8/938], Intermidiate Loss, Passive: 188.74935913085938, Active: 0.09194540232419968\n",
      "Epoch [0/10, 9/938], Intermidiate Loss, Passive: 229.35169982910156, Active: 0.06588846445083618\n",
      "Epoch [0/10, 10/938], Intermidiate Loss, Passive: 201.52981567382812, Active: 0.06800521910190582\n",
      "Epoch [0/10, 11/938], Intermidiate Loss, Passive: 224.52862548828125, Active: 0.06242571771144867\n",
      "Epoch [0/10, 12/938], Intermidiate Loss, Passive: 227.17987060546875, Active: 0.05266958847641945\n",
      "Epoch [0/10, 13/938], Intermidiate Loss, Passive: 206.5200958251953, Active: 0.05449843406677246\n",
      "Epoch [0/10, 14/938], Intermidiate Loss, Passive: 186.20989990234375, Active: 0.05973835289478302\n",
      "Epoch [0/10, 15/938], Intermidiate Loss, Passive: 183.71286010742188, Active: 0.060210950672626495\n",
      "Epoch [0/10, 16/938], Intermidiate Loss, Passive: 149.93821716308594, Active: 0.06480930000543594\n",
      "Epoch [0/10, 17/938], Intermidiate Loss, Passive: 175.63961791992188, Active: 0.05048011615872383\n",
      "Epoch [0/10, 18/938], Intermidiate Loss, Passive: 157.80079650878906, Active: 0.05692356452345848\n",
      "Epoch [0/10, 19/938], Intermidiate Loss, Passive: 158.3768310546875, Active: 0.05055498331785202\n",
      "Epoch [0/10, 20/938], Intermidiate Loss, Passive: 133.68069458007812, Active: 0.05080578103661537\n",
      "Epoch [0/10, 21/938], Intermidiate Loss, Passive: 146.57510375976562, Active: 0.045763809233903885\n",
      "Epoch [0/10, 22/938], Intermidiate Loss, Passive: 141.65870666503906, Active: 0.04679093882441521\n",
      "Epoch [0/10, 23/938], Intermidiate Loss, Passive: 132.69924926757812, Active: 0.046223949640989304\n",
      "Epoch [0/10, 24/938], Intermidiate Loss, Passive: 125.78081512451172, Active: 0.04388360679149628\n",
      "Epoch [0/10, 25/938], Intermidiate Loss, Passive: 122.38616943359375, Active: 0.04732459783554077\n",
      "Epoch [0/10, 26/938], Intermidiate Loss, Passive: 106.08872985839844, Active: 0.04908439517021179\n",
      "Epoch [0/10, 27/938], Intermidiate Loss, Passive: 131.86727905273438, Active: 0.04254993051290512\n",
      "Epoch [0/10, 28/938], Intermidiate Loss, Passive: 146.39730834960938, Active: 0.04152090474963188\n",
      "Epoch [0/10, 29/938], Intermidiate Loss, Passive: 155.90045166015625, Active: 0.03851885348558426\n",
      "Epoch [0/10, 30/938], Intermidiate Loss, Passive: 130.03028869628906, Active: 0.04015892744064331\n",
      "Epoch [0/10, 31/938], Intermidiate Loss, Passive: 138.563232421875, Active: 0.03817780315876007\n",
      "Epoch [0/10, 32/938], Intermidiate Loss, Passive: 125.72445678710938, Active: 0.04011602699756622\n",
      "Epoch [0/10, 33/938], Intermidiate Loss, Passive: 110.3886489868164, Active: 0.043019142001867294\n",
      "Epoch [0/10, 34/938], Intermidiate Loss, Passive: 103.80886840820312, Active: 0.03904460743069649\n",
      "Epoch [0/10, 35/938], Intermidiate Loss, Passive: 99.52894592285156, Active: 0.04036451876163483\n",
      "Epoch [0/10, 36/938], Intermidiate Loss, Passive: 110.24282836914062, Active: 0.037082064896821976\n",
      "Epoch [0/10, 37/938], Intermidiate Loss, Passive: 110.81974792480469, Active: 0.0378720685839653\n",
      "Epoch [0/10, 38/938], Intermidiate Loss, Passive: 109.83354949951172, Active: 0.039675261825323105\n",
      "Epoch [0/10, 39/938], Intermidiate Loss, Passive: 126.6875991821289, Active: 0.03392979875206947\n",
      "Epoch [0/10, 40/938], Intermidiate Loss, Passive: 132.0570068359375, Active: 0.032177675515413284\n",
      "Epoch [0/10, 41/938], Intermidiate Loss, Passive: 90.09309387207031, Active: 0.03866930678486824\n",
      "Epoch [0/10, 42/938], Intermidiate Loss, Passive: 110.14291381835938, Active: 0.035293277353048325\n",
      "Epoch [0/10, 43/938], Intermidiate Loss, Passive: 99.56489562988281, Active: 0.0356246717274189\n",
      "Epoch [0/10, 44/938], Intermidiate Loss, Passive: 86.36866760253906, Active: 0.03797627240419388\n",
      "Epoch [0/10, 45/938], Intermidiate Loss, Passive: 86.54769897460938, Active: 0.039398930966854095\n",
      "Epoch [0/10, 46/938], Intermidiate Loss, Passive: 105.04424285888672, Active: 0.03686163201928139\n",
      "Epoch [0/10, 47/938], Intermidiate Loss, Passive: 96.19914245605469, Active: 0.03521882742643356\n",
      "Epoch [0/10, 48/938], Intermidiate Loss, Passive: 81.2520751953125, Active: 0.036659497767686844\n",
      "Epoch [0/10, 49/938], Intermidiate Loss, Passive: 89.4005355834961, Active: 0.038047343492507935\n",
      "Epoch [0/10, 50/938], Intermidiate Loss, Passive: 83.73714447021484, Active: 0.036367516964673996\n",
      "Epoch [0/10, 51/938], Intermidiate Loss, Passive: 83.82440185546875, Active: 0.037639107555150986\n",
      "Epoch [0/10, 52/938], Intermidiate Loss, Passive: 100.26448059082031, Active: 0.03211226686835289\n",
      "Epoch [0/10, 53/938], Intermidiate Loss, Passive: 92.3502197265625, Active: 0.0326443575322628\n",
      "Epoch [0/10, 54/938], Intermidiate Loss, Passive: 96.20053100585938, Active: 0.033882226794958115\n",
      "Epoch [0/10, 55/938], Intermidiate Loss, Passive: 93.77372741699219, Active: 0.033663563430309296\n",
      "Epoch [0/10, 56/938], Intermidiate Loss, Passive: 79.8868408203125, Active: 0.033788371831178665\n",
      "Epoch [0/10, 57/938], Intermidiate Loss, Passive: 71.45693969726562, Active: 0.03558340668678284\n",
      "Epoch [0/10, 58/938], Intermidiate Loss, Passive: 76.69236755371094, Active: 0.03624546155333519\n",
      "Epoch [0/10, 59/938], Intermidiate Loss, Passive: 85.79219818115234, Active: 0.036696821451187134\n",
      "Epoch [0/10, 60/938], Intermidiate Loss, Passive: 86.15168762207031, Active: 0.03192738816142082\n",
      "Epoch [0/10, 61/938], Intermidiate Loss, Passive: 91.27780151367188, Active: 0.03516150265932083\n",
      "Epoch [0/10, 62/938], Intermidiate Loss, Passive: 90.10870361328125, Active: 0.0351572223007679\n",
      "Epoch [0/10, 63/938], Intermidiate Loss, Passive: 91.45040893554688, Active: 0.03377780690789223\n",
      "Epoch [0/10, 64/938], Intermidiate Loss, Passive: 93.6330337524414, Active: 0.030977802351117134\n",
      "Epoch [0/10, 65/938], Intermidiate Loss, Passive: 89.41775512695312, Active: 0.03170280158519745\n",
      "Epoch [0/10, 66/938], Intermidiate Loss, Passive: 65.7988510131836, Active: 0.035584114491939545\n",
      "Epoch [0/10, 67/938], Intermidiate Loss, Passive: 82.63139343261719, Active: 0.030209675431251526\n",
      "Epoch [0/10, 68/938], Intermidiate Loss, Passive: 76.58563232421875, Active: 0.029929501935839653\n",
      "Epoch [0/10, 69/938], Intermidiate Loss, Passive: 91.69503021240234, Active: 0.02876344509422779\n",
      "Epoch [0/10, 70/938], Intermidiate Loss, Passive: 65.59683227539062, Active: 0.03237190842628479\n",
      "Epoch [0/10, 71/938], Intermidiate Loss, Passive: 92.25469970703125, Active: 0.030300311744213104\n",
      "Epoch [0/10, 72/938], Intermidiate Loss, Passive: 69.30374145507812, Active: 0.032896578311920166\n",
      "Epoch [0/10, 73/938], Intermidiate Loss, Passive: 74.78178405761719, Active: 0.03289075940847397\n",
      "Epoch [0/10, 74/938], Intermidiate Loss, Passive: 74.33026123046875, Active: 0.034975700080394745\n",
      "Epoch [0/10, 75/938], Intermidiate Loss, Passive: 59.98762893676758, Active: 0.03356867656111717\n",
      "Epoch [0/10, 76/938], Intermidiate Loss, Passive: 90.03012084960938, Active: 0.028239769861102104\n",
      "Epoch [0/10, 77/938], Intermidiate Loss, Passive: 68.7526626586914, Active: 0.03309120237827301\n",
      "Epoch [0/10, 78/938], Intermidiate Loss, Passive: 73.31800842285156, Active: 0.02913421206176281\n",
      "Epoch [0/10, 79/938], Intermidiate Loss, Passive: 69.0252456665039, Active: 0.029372429475188255\n",
      "Epoch [0/10, 80/938], Intermidiate Loss, Passive: 71.93302917480469, Active: 0.027897462248802185\n",
      "Epoch [0/10, 81/938], Intermidiate Loss, Passive: 63.894386291503906, Active: 0.02717031165957451\n",
      "Epoch [0/10, 82/938], Intermidiate Loss, Passive: 59.76576232910156, Active: 0.02963145077228546\n",
      "Epoch [0/10, 83/938], Intermidiate Loss, Passive: 67.07197570800781, Active: 0.031118396669626236\n",
      "Epoch [0/10, 84/938], Intermidiate Loss, Passive: 66.03875732421875, Active: 0.028814729303121567\n",
      "Epoch [0/10, 85/938], Intermidiate Loss, Passive: 68.71414184570312, Active: 0.027788834646344185\n",
      "Epoch [0/10, 86/938], Intermidiate Loss, Passive: 63.0345573425293, Active: 0.0270384568721056\n",
      "Epoch [0/10, 87/938], Intermidiate Loss, Passive: 73.60234069824219, Active: 0.027214979752898216\n",
      "Epoch [0/10, 88/938], Intermidiate Loss, Passive: 72.31875610351562, Active: 0.027981465682387352\n",
      "Epoch [0/10, 89/938], Intermidiate Loss, Passive: 82.05003356933594, Active: 0.026316184550523758\n",
      "Epoch [0/10, 90/938], Intermidiate Loss, Passive: 77.60001373291016, Active: 0.0242311991751194\n",
      "Epoch [0/10, 91/938], Intermidiate Loss, Passive: 65.70606994628906, Active: 0.026679811999201775\n",
      "Epoch [0/10, 92/938], Intermidiate Loss, Passive: 70.29771423339844, Active: 0.026408236473798752\n",
      "Epoch [0/10, 93/938], Intermidiate Loss, Passive: 65.08930206298828, Active: 0.0267005804926157\n",
      "Epoch [0/10, 94/938], Intermidiate Loss, Passive: 64.66746520996094, Active: 0.027420664206147194\n",
      "Epoch [0/10, 95/938], Intermidiate Loss, Passive: 64.29269409179688, Active: 0.02825053036212921\n",
      "Epoch [0/10, 96/938], Intermidiate Loss, Passive: 69.93293762207031, Active: 0.024458574131131172\n",
      "Epoch [0/10, 97/938], Intermidiate Loss, Passive: 61.403839111328125, Active: 0.02769405208528042\n",
      "Epoch [0/10, 98/938], Intermidiate Loss, Passive: 53.16771697998047, Active: 0.029773613438010216\n",
      "Epoch [0/10, 99/938], Intermidiate Loss, Passive: 63.50810241699219, Active: 0.026006590574979782\n",
      "Epoch [0/10, 100/938], Intermidiate Loss, Passive: 72.31301879882812, Active: 0.026642346754670143\n",
      "Epoch [0/10, 101/938], Intermidiate Loss, Passive: 75.16331481933594, Active: 0.025912614539265633\n",
      "Epoch [0/10, 102/938], Intermidiate Loss, Passive: 63.69449234008789, Active: 0.02588181011378765\n",
      "Epoch [0/10, 103/938], Intermidiate Loss, Passive: 59.19178009033203, Active: 0.027534838765859604\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m loss_act\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     64\u001b[0m optimizer_pas\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 65\u001b[0m \u001b[43moptimizer_act\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#scaler.scale(loss_pas).backward()\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#scaler.unscale_(optimizer_pas)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_norm_(model_pas.parameters(), max_norm=10.0)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#scaler.step(optimizer_act)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#scaler.update()\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdat_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Intermidiate Loss, Passive: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_pas\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Active: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_act\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\optim\\adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    217\u001b[0m         group,\n\u001b[0;32m    218\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m         state_steps,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\optim\\optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\optim\\adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\optim\\adam.py:584\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    582\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[0;32m    586\u001b[0m     ]\n\u001b[0;32m    587\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    588\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[0;32m    589\u001b[0m     ]\n\u001b[0;32m    591\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\optim\\adam.py:585\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    582\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    584\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 585\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[0;32m    586\u001b[0m     ]\n\u001b[0;32m    587\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    588\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[0;32m    589\u001b[0m     ]\n\u001b[0;32m    591\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\optim\\optimizer.py:104\u001b[0m, in \u001b[0;36m_get_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# 5. Initialize Model & Train with Mini-batch SGD\n",
    "# ====================================\n",
    "# Define the optimizer (SGD instead of Adam)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)  # ðŸ”¥ SGD with Momentum\n",
    "device = \"cuda\"\n",
    "optimizer_pas = optim.Adam(model_pas.parameters(), lr=1e-3)  # ðŸ”¥ SGD with Momentum\n",
    "optimizer_act = optim.Adam(model_act.parameters(), lr=1e-3)\n",
    "\n",
    "#scaler = torch.cuda.amp.GradScaler()  # ðŸ”¥ Mixed precision training\n",
    "#scaler = torch.amp.GradScaler(\"cuda\")\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "T = torch.tensor(1.0, device=device)\n",
    "Tp = torch.tensor(1e-3, device=device)\n",
    "Ta = torch.tensor(1.0, device=device)\n",
    "tau = torch.tensor(0.5, device=device)\n",
    "k = torch.tensor(1.0, device=device)\n",
    "\n",
    "# Create Mini-batches using DataLoader\n",
    "nsamples = len(train_dataset)\n",
    "batch_size = 64 # Define mini-batch size\n",
    "num_epochs = 10\n",
    "import torch.amp as amp\n",
    "dataloader = train_loader\n",
    "\n",
    "# Use the new API, specifying the device type explicitly.\n",
    "#scaler = amp.GradScaler(\"cuda\")\n",
    "scaler = amp.GradScaler(device)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_pas = 0.0\n",
    "    total_loss_act = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    # Generate Gaussian noise with shape (nsamples, 28, 28)\n",
    "    gaussian_data_eta = np.random.normal(\n",
    "        loc=0,  # Mean\n",
    "        scale=np.sqrt(Ta.cpu().numpy() / tau.cpu().numpy()),  # Standard deviation\n",
    "        size=(nsamples, 28, 28)  # Shape: (nsamples, 28, 28)\n",
    "    )\n",
    "\n",
    "    g_tensor_eta = torch.tensor(gaussian_data_eta, dtype=torch.float32).view(-1, 1, 28, 28)\n",
    "    # Wrap it back into a TensorDataset\n",
    "    dataset_eta = TensorDataset(g_tensor_eta)\n",
    "    dataloader_eta = DataLoader(dataset_eta, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for (x0_batch, _), (eta0_batch), dat_idx in zip(dataloader, dataloader_eta, range(len(train_loader))):\n",
    "        x0 = x0_batch.to(device=device)\n",
    "        eta0 = eta0_batch[0].to(device=device)\n",
    "        t = 1e-3 + (1.0 - 1e-3) * torch.rand(x0.shape[0], device=device)\n",
    "        t = t.view(-1,1)\n",
    "        #t = torch.rand(x0.shape[0], device=device)\n",
    "        # Use the new autocast API with the device specified.\n",
    "        with torch.amp.autocast(device):\n",
    "            loss_pas = diffusion_loss(model_pas, x0, T, t)\n",
    "            loss_act = diffusion_loss_active(model_act, x0, eta0, Tp, Ta, tau, k, t)\n",
    "\n",
    "        if torch.isnan(loss_pas) or torch.isnan(loss_act):\n",
    "            print(f\"âš ï¸ Loss became NaN at epoch {epoch}, stopping training!\")\n",
    "            break\n",
    "\n",
    "        optimizer_pas.zero_grad()\n",
    "        optimizer_act.zero_grad()\n",
    "        loss_pas.backward()\n",
    "        loss_act.backward()\n",
    "        optimizer_pas.step()\n",
    "        optimizer_act.step()\n",
    "        #scaler.scale(loss_pas).backward()\n",
    "        #scaler.unscale_(optimizer_pas)\n",
    "        #torch.nn.utils.clip_grad_norm_(model_pas.parameters(), max_norm=10.0)\n",
    "        #scaler.step(optimizer_pas)\n",
    "        #scaler.scale(loss_act).backward()\n",
    "        #scaler.unscale_(optimizer_act)\n",
    "        #torch.nn.utils.clip_grad_norm_(model_act.parameters(), max_norm=10.0)\n",
    "        #scaler.step(optimizer_act)\n",
    "        #scaler.update()\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}, {dat_idx}/{len(train_loader)}], Intermidiate Loss, Passive: {loss_pas.item()}, Active: {loss_act.item()}\")\n",
    "        total_loss_pas += loss_pas.item()\n",
    "        total_loss_act += loss_act.item()\n",
    "\n",
    "        if (dat_idx+1)%10 == 0:\n",
    "            torch.save({\n",
    "            'model_state_dict': model_pas.state_dict(),  # Model weights\n",
    "            'epoch': epoch,  # Epoch\n",
    "        }, 'Passive_models_T_{}/model_at_epoch_{}_datidx_{}.pth'.format(T, epoch, dat_idx))\n",
    "        if (dat_idx+1)%10 == 0:\n",
    "            torch.save({\n",
    "            'model_state_dict': model_act.state_dict(),  # Model weights\n",
    "            'epoch': epoch,  # Epoch\n",
    "        }, 'Active_models_Ta_{}_tau_{}/model_at_epoch_{}_datidx_{}.pth'.format(Ta, tau, epoch, dat_idx))\n",
    "\n",
    "    avg_loss_pas = total_loss_pas / num_batches\n",
    "    avg_loss_act = total_loss_act / num_batches\n",
    "    avg_loss_act = 0.0\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Average Loss, Passive: {avg_loss_pas:.6f}, Active: {avg_loss_act:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35fe6c-f892-49c6-9350-1d31363f6275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
